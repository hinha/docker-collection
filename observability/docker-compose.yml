x-default-logging: &logging
  driver: "json-file"
  options:
    max-size: "5m"
    max-file: "2"
    tag: "{{.Name}}"

networks:
  opentelemetry:
    external: true
    driver: bridge

services:
  otel-collector:
    networks:
      - opentelemetry
    image: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib:0.120.0
    container_name: otel-collector
    deploy:
      resources:
        limits:
          memory: 200M
    restart: unless-stopped
    command: [ "--config=/etc/otelcol-config.yml", "--config=/etc/otelcol-config-extras.yml" ]
    user: 0:0
    volumes:
      - /:/hostfs:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./src/otel-collector/otelcol-config.yml:/etc/otelcol-config.yml
      - ./src/otel-collector/otelcol-config-extras.yml:/etc/otelcol-config-extras.yml
    ports:
      - "4317:4317"  # ✅ gRPC (OTLP)
      - "4318:4318"  # ✅ HTTP (OTLP Metrics)
      - "8889:8889"  # ✅ Prometheus /metrics
      - "55681:55681" # (optional legacy ports)
    depends_on:
      tempo:
        condition: service_started
    healthcheck:
      test: ["CMD", "grpcurl", "-plaintext", "localhost:4317", "grpc.health.v1.Health/Check"]
      interval: 10s
      timeout: 5s
      retries: 5
    logging: *logging
    environment:
      # - ENVOY_PORT=8080
      - OST_FILESYSTEM=/
      - OTEL_COLLECTOR_HOST=otel-collector
      - OTEL_COLLECTOR_PORT_GRPC=4317
      - OTEL_COLLECTOR_PORT_HTTP=4318
      - GOMEMLIMIT=160MiB
  grafana:
    platform: linux/amd64
    networks:
      - opentelemetry
    container_name: grafana
    image: grafana/grafana-oss:12.0.0
    deploy:
      resources:
        limits:
          memory: 120M
    restart: unless-stopped
    environment:
      - "GF_INSTALL_PLUGINS=grafana-opensearch-datasource,grafana-clock-panel,alexanderzobnin-zabbix-app"
      - "GF_SECURITY_ADMIN_USER=admin@example.com"
      - "GF_SECURITY_ADMIN_PASSWORD=supersecure123"
    volumes:
      - ./src/grafana/grafana.ini:/etc/grafana/grafana.ini
      - ./src/grafana/provisioning/:/etc/grafana/provisioning/
    ports:
      - "3000:3000"
    logging: *logging
  
  prometheus:
    platform: linux/amd64
    networks:
      - opentelemetry
    image: quay.io/prometheus/prometheus:v3.2.0
    container_name: prometheus
    command:
      - --web.console.templates=/etc/prometheus/consoles
      - --web.console.libraries=/etc/prometheus/console_libraries
      - --storage.tsdb.retention.time=1h
      - --config.file=/etc/prometheus/prometheus-config.yaml
      - --storage.tsdb.path=/prometheus
      - --web.enable-lifecycle
      - --web.route-prefix=/
      - --web.enable-otlp-receiver
      - --enable-feature=exemplar-storage
    volumes:
      - prometheus-data:/etc/prometheus
      - ./src/prometheus/prometheus-config.yaml:/etc/prometheus/prometheus-config.yaml
    deploy:
      resources:
        limits:
          memory: 300M
    restart: unless-stopped
    ports:
      - "9090:9090"
    logging: *logging

  tempo:
    image: grafana/tempo:2.7.2
    platform: linux/amd64
    container_name: tempo
    user: 0:0
    networks:
      - opentelemetry
    restart: unless-stopped
    volumes:
      - ./src/tempo/tempo.yaml:/etc/tempo.yaml
      - tempo-data:/var/lib/tempo
      - tempo-data:/persisted
    command: [ "-config.file=/etc/tempo.yaml", "-log.level=debug" ]
    depends_on:
      tempo-cache:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-f", "http://tempo:3200/ready"]
      interval: 10s
      timeout: 5s
      retries: 5
    ports:
      - "3200:3200" # Tempo UI
      - "4319:4319"
    environment:
      - TZ=Asia/Jakarta
      - TEMPO_QUERIER_CACHE_BACKEND=redis
      - TEMPO_QUERIER_CACHE_REDIS_ENDPOINT=host.docker.internal:6379
    logging: *logging

volumes:
  prometheus-data:
    driver: local
  tempo-data:
    driver: local